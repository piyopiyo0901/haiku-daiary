#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import re
import time
import hashlib
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import pyperclip
import keyboard
from janome.tokenizer import Tokenizer


# =========================
# è¨­å®š
# =========================

INBOX_DIR = r"C:\Users\zyaga\OneDrive\Documents\ObsidianVault\00_INBOX"

FIXED_TAGS = ["INBOX", "clip", "idea"]

# å·¦æ‰‹ã§å±Šããƒ›ãƒƒãƒˆã‚­ãƒ¼
HOTKEY = "ctrl+alt+q"

MIN_CHARS = 3

# å±¥æ­´ãƒ¡ãƒ³ãƒ†ï¼ˆè‚¥å¤§åŒ–é˜²æ­¢ï¼‰
DEDUPE_MAX_RECORDS = 2000
DEDUPE_HISTORY_FILENAME = "_clip_history.json"

# ãƒ•ã‚¡ã‚¤ãƒ«åè¦ç´„ã®æœ€å¤§é•·ï¼ˆã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¾Œï¼‰
FILENAME_SUMMARY_MAX = 40

# =========================
# åˆ†é¡ãƒ«ãƒ¼ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¢—ã‚„ã›ã¾ã™ï¼‰
# =========================

CATEGORY_RULES: Dict[str, Dict[str, List[str]]] = {
    "work": {
        "any": [
            "ä¼šè­°", "è­°äº‹éŒ²", "è¦ä»¶", "è¨­è¨ˆ", "ãƒ†ã‚¹ãƒˆ", "éšœå®³", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ä»•æ§˜", "èª²é¡Œ", "ã‚¿ã‚¹ã‚¯",
            "WBS", "é€²æ—", "é¡§å®¢", "ãƒ¦ãƒ¼ã‚¶", "å•ã„åˆã‚ã›", "ãƒªãƒªãƒ¼ã‚¹", "ä¿å®ˆ", "é‹ç”¨",
            "SVN", "Git", "Bitbucket", "Jira", "Confluence", "SharePoint", "OneNote", "Copilot", "Teams",
            "è¦‹ç©", "ç¨¼åƒ", "å·¥æ•°", "PR", "MR", "ãƒ–ãƒ©ãƒ³ãƒ"
        ],
        "all": []
    },
    "shopping": {
        "any": ["è²·ã†", "è³¼å…¥", "æ³¨æ–‡", "Amazon", "æ¥½å¤©", "ä¾¡æ ¼", "ã‚»ãƒ¼ãƒ«", "æ¯”è¼ƒ", "ã‚¯ãƒ¼ãƒãƒ³", "ãƒã‚¤ãƒ³ãƒˆ", "åœ¨åº«"],
        "all": []
    },
    "health": {
        "any": ["ä½“é‡", "ç­‹ãƒˆãƒ¬", "ã‚¸ãƒ ", "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«", "ç¡çœ ", "ç–²ã‚Œ", "ä½“èª¿", "é£Ÿäº‹", "PFC", "ã‚¿ãƒ³ãƒ‘ã‚¯", "ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ"],
        "all": []
    },
    "game": {
        "any": ["ãƒ¢ãƒ³ãƒãƒ³", "ãƒ¢ãƒ³ãƒãƒ³NOW", "Switch", "Steam", "PS5", "æ”»ç•¥", "å‘¨å›", "ãƒ¬ãƒ™ãƒ«", "ã‚¬ãƒãƒ£", "ãƒœã‚¹", "ã‚¯ã‚¨ã‚¹ãƒˆ"],
        "all": []
    },
    "travel": {
        "any": ["æ—…è¡Œ", "ãƒ›ãƒ†ãƒ«", "æ–°å¹¹ç·š", "é£›è¡Œæ©Ÿ", "äºˆç´„", "ãƒ«ãƒ¼ãƒˆ", "è¦³å…‰", "æ¸©æ³‰", "é§…", "ç©ºæ¸¯"],
        "all": []
    },
    "finance": {
        "any": ["å®¶è¨ˆ", "ç¨é‡‘", "ãµã‚‹ã•ã¨ç´ç¨", "ã‚¯ãƒ¬ã‚«", "ãƒã‚¤ãƒ³ãƒˆ", "æ”¯å‡º", "è²¯é‡‘", "æŠ•è³‡"],
        "all": []
    },
    "obsidian": {
        "any": ["Obsidian", "Vault", "Daily", "diary", "ãƒªãƒ³ã‚¯", "[[", "ã‚¿ã‚°", "ãƒ†ãƒ³ãƒ—ãƒ¬", "Second Brain"],
        "all": []
    },
}

# wikilinkã¯ã€Œæœ¬æ–‡ã®èª­ã¿ã‚„ã™ã•å„ªå…ˆã€ã§ä¸Šé™ã¤ã
MAX_WIKILINKS = 12
BASE_WIKILINK_SEEDS = [
    "Obsidian", "Second Brain", "INBOX", "Daily Note", "ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«", "æŒ¯ã‚Šè¿”ã‚Š",
    "æ°—ã¥ããƒ¡ãƒ¢", "ã‚¿ã‚¹ã‚¯ç®¡ç†", "è‡ªå‹•åŒ–", "Python", "Copilot", "Copilot 365",
]

JP_STOPWORDS = set([
    "ã“ã¨", "ã‚‚ã®", "ãã‚Œ", "ã“ã‚Œ", "ãŸã‚", "ã¨ã“ã‚", "æ„Ÿã˜", "è‡ªåˆ†", "ä»Šæ—¥", "æ˜æ—¥",
    "ä»Šå›", "ä¸€æ—¦", "å¿…è¦", "å¯èƒ½", "æ–¹", "æ™‚", "ã‚ã¨", "å‰", "ä¸­", "å¾Œ", "ä¸Š", "ä¸‹",
    "ç§", "åƒ•", "ä¿º", "ã‚ãªãŸ", "ã•ã‚“", "çš„", "ä»–", "ãªã©"
])

tokenizer = Tokenizer()


@dataclass
class DedupeRecord:
    sha256: str
    created_at: str
    filename: str


# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================

def now_str() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def normalize_text(text: str) -> str:
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()


def sha256_of(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def atomic_write_text(path: Path, content: str, encoding: str = "utf-8") -> None:
    tmp = path.with_suffix(path.suffix + ".tmp")
    tmp.write_text(content, encoding=encoding)
    tmp.replace(path)  # atomic on same filesystem


# =========================
# å±¥æ­´ï¼ˆ8: è‡ªå‹•ãƒ¡ãƒ³ãƒ†ï¼‰
# =========================

def load_history(path: Path) -> List[DedupeRecord]:
    if not path.exists():
        return []
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        out: List[DedupeRecord] = []
        for item in data.get("records", []):
            sha = item.get("sha256", "")
            if not sha:
                continue
            out.append(DedupeRecord(
                sha256=sha,
                created_at=item.get("created_at", ""),
                filename=item.get("filename", ""),
            ))
        return out
    except Exception:
        # å£Šã‚Œã¦ã„ã¦ã‚‚èµ·å‹•ã¯ç¶™ç¶šï¼ˆæ–°è¦å±¥æ­´æ‰±ã„ï¼‰
        return []


def maintenance_history(records: List[DedupeRecord]) -> List[DedupeRecord]:
    """
    - åŒä¸€shaã®é‡è¤‡æ’é™¤ï¼ˆæœ€æ–°å„ªå…ˆï¼‰
    - ä¸Šé™ãƒˆãƒªãƒ ï¼ˆæœ«å°¾ãŒæœ€æ–°ã«ãªã‚‹å‰æã§ä¿æŒï¼‰
    """
    # created_atã§ä¸¦ã‚“ã§ãªã„å¯èƒ½æ€§ã‚‚ã‚ã‚‹ã®ã§ã€å…¥åŠ›é †ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«é †ï¼‰ã‚’å°Šé‡ã—ã¤ã¤é‡è¤‡æ’é™¤
    seen = set()
    compacted: List[DedupeRecord] = []
    # å¾Œã‚ã‹ã‚‰è¦‹ã¦ã€åˆã‚ã¦å‡ºã‚‹shaã ã‘æ®‹ã™ = æœ€æ–°å„ªå…ˆ
    for r in reversed(records):
        if r.sha256 in seen:
            continue
        seen.add(r.sha256)
        compacted.append(r)
    compacted.reverse()

    if len(compacted) > DEDUPE_MAX_RECORDS:
        compacted = compacted[-DEDUPE_MAX_RECORDS:]
    return compacted


def save_history(path: Path, records: List[DedupeRecord]) -> None:
    records = maintenance_history(records)
    payload = {
        "updated_at": now_str(),
        "records": [r.__dict__ for r in records],
    }
    atomic_write_text(path, json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


# =========================
# åˆ†é¡ãƒ»ãƒªãƒ³ã‚¯å€™è£œ
# =========================

def detect_categories(text: str) -> List[str]:
    tags = []
    for tag, rule in CATEGORY_RULES.items():
        any_hits = any(k.lower() in text.lower() for k in rule.get("any", []))
        all_req = rule.get("all", [])
        all_hits = all(k.lower() in text.lower() for k in all_req) if all_req else True
        if any_hits and all_hits:
            tags.append(tag)
    return tags


def extract_english_terms(text: str) -> List[str]:
    candidates = re.findall(r"[A-Za-z0-9][A-Za-z0-9\._\-/\+]{1,}", text)
    out = []
    for c in candidates:
        if len(c) < 3:
            continue
        if re.fullmatch(r"\d+", c):
            continue
        out.append(c)
    return out


def extract_japanese_nouns(text: str) -> List[str]:
    out = []
    for token in tokenizer.tokenize(text):
        base = token.base_form
        pos = token.part_of_speech.split(",")
        if pos[0] != "åè©":
            continue
        if pos[1] in ["æ•°", "éè‡ªç«‹", "ä»£åè©"]:
            continue
        if not base or base in JP_STOPWORDS:
            continue
        if len(base) <= 1:
            continue
        out.append(base)
    return out


def score_terms(text: str, terms: List[str]) -> List[Tuple[str, int]]:
    scored = {}
    for term in terms:
        freq = len(re.findall(re.escape(term), text))
        if freq <= 0:
            continue
        score = freq * 10 + min(len(term), 12)
        scored[term] = max(scored.get(term, 0), score)
    return sorted(scored.items(), key=lambda x: x[1], reverse=True)


def select_wikilinks(text: str) -> List[str]:
    """
    æœ¬æ–‡ã«ãƒªãƒ³ã‚¯ã¯å…¥ã‚Œãªã„ã€‚
    æœ«å°¾ã«å‡ºã™ã€Œå€™è£œä¸€è¦§ã€ç”¨ãªã®ã§ã€ãƒã‚¤ã‚ºã‚’æ¸›ã‚‰ã™ãŸã‚ã«
    - 4æ–‡å­—ä»¥ä¸‹ã®çŸ­ã„èªã¯åŸå‰‡é™¤å¤–ï¼ˆseedã¯ä¾‹å¤–ï¼‰
    - å‡ºç¾2å›ä»¥ä¸Šã‚’å„ªå…ˆï¼ˆ1å›èªã¯å³ã—ã‚ï¼‰
    """
    seeds = [s for s in BASE_WIKILINK_SEEDS if s and s.lower() in text.lower()]
    nouns = extract_japanese_nouns(text)
    eng = extract_english_terms(text)

    candidates = list(set(seeds + nouns + eng))
    ranked = score_terms(text, candidates)

    selected = []
    for term, _score in ranked:
        freq = len(re.findall(re.escape(term), text))

        # ãƒã‚¤ã‚ºæŠ‘åˆ¶ï¼ˆå€™è£œä¸€è¦§ã®å“è³ªå„ªå…ˆï¼‰
        if term not in seeds:
            if len(term) <= 4:
                continue
            if freq < 2:
                continue

        if term in JP_STOPWORDS:
            continue

        selected.append(term)
        if len(selected) >= MAX_WIKILINKS:
            break

    return sorted(set(selected), key=len, reverse=True)



def linkify_text(body: str, keywords: List[str]) -> str:
    if not keywords:
        return body

    protected: List[str] = []

    def protect(pattern: str, s: str) -> str:
        def _p(m):
            protected.append(m.group(0))
            return f"__PROTECTED_{len(protected)-1}__"
        return re.sub(pattern, _p, s, flags=re.DOTALL)

    tmp = body
    tmp = protect(r"```.*?```", tmp)
    tmp = protect(r"`[^`]*`", tmp)
    tmp = protect(r"\[\[[^\]]+\]\]", tmp)

    for kw in keywords:
        pattern = re.compile(rf"(?<!\[)(?<!#)(?<!_){re.escape(kw)}")
        tmp = pattern.sub(f"[[{kw}]]", tmp)

    for i, original in enumerate(protected):
        tmp = tmp.replace(f"__PROTECTED_{i}__", original)

    return tmp


# =========================
# 10: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚«ãƒ†ã‚´ãƒª+è¦ç´„ï¼‰
# =========================

SENSITIVE_PATTERNS = [
    # email
    (re.compile(r"[A-Za-z0-9._%+\-]+@[A-Za-z0-9.\-]+\.[A-Za-z]{2,}"), "<email>"),
    # phone-like (very rough)
    (re.compile(r"\b0\d{1,3}-\d{2,4}-\d{3,4}\b"), "<phone>"),
    # long digit sequences (IDs, ticket numbers etc.)
    (re.compile(r"\b\d{4,}\b"), "<num>"),
]

def sanitize_filename_part(s: str) -> str:
    s = s.strip()
    s = re.sub(r"[\\/:*?\"<>|]+", "_", s)
    s = re.sub(r"\s+", " ", s).strip()
    s = s.strip(".")
    return s


def redact_sensitive(s: str) -> str:
    out = s
    for pat, rep in SENSITIVE_PATTERNS:
        out = pat.sub(rep, out)
    return out


def make_summary_for_filename(text: str, fallback_terms: Optional[List[str]] = None) -> str:
    """
    - 1è¡Œç›®ãŒæœ‰ç”¨ãªã‚‰ãã‚Œã‚’ä¸»ã«ä½¿ã†
    - ã ã‚ãªã‚‰åè©ä¸Šä½ã‚’é€£çµ
    - æ©Ÿå¯†ã£ã½ã„ã‚‚ã®ï¼ˆemail/é•·ã„æ•°å­—ï¼‰ã¯ãƒã‚¹ã‚¯
    """
    t = normalize_text(text)
    lines = [ln.strip() for ln in t.split("\n") if ln.strip()]
    first = lines[0] if lines else ""

    # 1è¡Œç›®ãŒé•·ã™ãã‚‹/URLã ã‚‰ã‘ç­‰ãªã‚‰è½ã¨ã™
    if first:
        if first.startswith("http") or len(first) < 4:
            first = ""
        # URLå«æœ‰ãŒå¼·ã„ãªã‚‰æ¨ã¦ã‚‹
        if re.search(r"https?://", first):
            first = ""

    if first:
        cand = first
    else:
        # åè©+è‹±èªã‹ã‚‰è¦ç´„
        terms = fallback_terms or (extract_japanese_nouns(t) + extract_english_terms(t))
        # é‡è¦ã£ã½ã„é †ï¼šå‡ºç¾å›æ•°Ã—é•·ã•
        ranked = [w for w, _ in score_terms(t, list(set(terms)))]
        cand = " ".join(ranked[:6]) if ranked else "clip"

    cand = redact_sensitive(cand)
    cand = sanitize_filename_part(cand)

    if len(cand) > FILENAME_SUMMARY_MAX:
        cand = cand[:FILENAME_SUMMARY_MAX].rstrip()

    return cand if cand else "clip"


def choose_primary_category(auto_categories: List[str]) -> str:
    return auto_categories[0] if auto_categories else "misc"


# =========================
# ãƒãƒ¼ãƒˆæœ¬æ–‡ç”Ÿæˆ
# =========================

def to_bullets(text: str) -> str:
    lines = text.split("\n")
    out = []
    for line in lines:
        raw = line.strip()
        if not raw:
            continue
        if raw.startswith(("- ", "* ", "- [ ]", "- [x]", "- [X]")):
            out.append(raw)
        else:
            out.append(f"- {raw}")
    return "\n".join(out) if out else "- "


def build_markdown(raw_text: str, tags: List[str], wikilinks: List[str]) -> str:
    dt = datetime.now()
    date_str = dt.strftime("%Y-%m-%d")
    time_str = dt.strftime("%H:%M:%S")

    clean = normalize_text(raw_text)
    bullets = to_bullets(clean)

    tags_yaml = "\n".join([f"  - {t.lstrip('#')}" for t in tags])

    frontmatter = f"""---
created: {date_str} {time_str}
tags:
{tags_yaml}
source: clipboard
---
"""

    # æœ«å°¾ã«ã ã‘å€™è£œä¸€è¦§ã‚’å‡ºã™ï¼ˆæœ¬æ–‡ã¯ãƒªãƒ³ã‚¯åŒ–ã—ãªã„ï¼‰
    if wikilinks:
        link_lines = "\n".join([f"- [[{w}]]" for w in wikilinks])
    else:
        link_lines = "- "  # ç©ºã ã¨è¦‹æ „ãˆãŒæ‚ªã„ã®ã§ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€

    body = f"""
# ğŸ“¥ INBOXã‚¯ãƒªãƒƒãƒ— ({date_str})

## å†…å®¹
{bullets}

## ğŸ”— ãƒªãƒ³ã‚¯å€™è£œ
{link_lines}

## ãƒ¡ã‚¿
- ä¿å­˜: {date_str} {time_str}
""".lstrip()

    # â˜…é‡è¦ï¼šã“ã“ã§ linkify_text() ã¯å‘¼ã°ãªã„ï¼ˆæœ¬æ–‡ã‚’æ±šã•ãªã„ï¼‰
    return frontmatter + "\n" + body


def write_note(inbox_dir: Path, md: str, raw_text: str, category: str, summary: str) -> Path:
    inbox_dir.mkdir(parents=True, exist_ok=True)

    dt = datetime.now()
    ts = dt.strftime("%Y-%m-%d_%H-%M-%S")

    category_part = sanitize_filename_part(category)
    summary_part = sanitize_filename_part(summary)

    filename = f"{ts}_{category_part}_{summary_part}.md"
    path = inbox_dir / filename

    i = 1
    while path.exists():
        path = inbox_dir / f"{ts}_{category_part}_{summary_part}_{i}.md"
        i += 1

    path.write_text(md, encoding="utf-8")
    return path


# =========================
# å®Ÿè¡Œï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å±¥æ­´ä¿æŒï¼‰
# =========================

INBOX_PATH = Path(INBOX_DIR)
HISTORY_PATH = INBOX_PATH / DEDUPE_HISTORY_FILENAME

HISTORY_RECORDS: List[DedupeRecord] = []
HISTORY_HASHES = set()


def init_history():
    global HISTORY_RECORDS, HISTORY_HASHES
    INBOX_PATH.mkdir(parents=True, exist_ok=True)

    HISTORY_RECORDS = load_history(HISTORY_PATH)
    HISTORY_RECORDS = maintenance_history(HISTORY_RECORDS)
    HISTORY_HASHES = set(r.sha256 for r in HISTORY_RECORDS)

    # ãƒ¡ãƒ³ãƒ†çµæœã‚’å³ä¿å­˜ï¼ˆå£Šã‚Œ/é‡è¤‡/è‚¥å¤§åŒ–ã®ä¿®å¾©ï¼‰
    save_history(HISTORY_PATH, HISTORY_RECORDS)


def handle_hotkey():
    global HISTORY_RECORDS, HISTORY_HASHES

    try:
        raw = pyperclip.paste()
    except Exception:
        print("[skip] cannot read clipboard")
        return

    if not isinstance(raw, str):
        print("[skip] clipboard is not text")
        return

    raw_norm = normalize_text(raw)
    if len(raw_norm) < MIN_CHARS:
        print("[skip] too short")
        return

    h = sha256_of(raw_norm)
    if h in HISTORY_HASHES:
        print("[skip] duplicate (already saved)")
        return

    auto_categories = detect_categories(raw_norm)
    category = choose_primary_category(auto_categories)

    # tags: å›ºå®š + è‡ªå‹•ã‚«ãƒ†ã‚´ãƒª
    tags = list(dict.fromkeys(FIXED_TAGS + auto_categories))

    wikilinks = select_wikilinks(raw_norm)

    # ãƒ•ã‚¡ã‚¤ãƒ«åç”¨ã®è¦ç´„ï¼ˆæœ¬æ–‡ã‹ã‚‰ç”Ÿæˆï¼‰
    summary = make_summary_for_filename(raw_norm)

    md = build_markdown(raw_norm, tags, wikilinks)
    saved = write_note(INBOX_PATH, md, raw_norm, category, summary)

    # å±¥æ­´æ›´æ–°ï¼ˆãƒ¡ãƒ¢ãƒª + ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    rec = DedupeRecord(sha256=h, created_at=now_str(), filename=saved.name)
    HISTORY_RECORDS.append(rec)
    HISTORY_RECORDS = maintenance_history(HISTORY_RECORDS)
    HISTORY_HASHES = set(r.sha256 for r in HISTORY_RECORDS)
    save_history(HISTORY_PATH, HISTORY_RECORDS)

    print(f"[saved] {saved}")


def main():
    init_history()

    print("=== Clipboard -> Obsidian INBOX (Hotkey Save) ===")
    print(f"INBOX_DIR : {INBOX_DIR}")
    print(f"HOTKEY    : {HOTKEY}")
    print(f"History   : {HISTORY_PATH} (max {DEDUPE_MAX_RECORDS})")
    print("ãƒ›ãƒƒãƒˆã‚­ãƒ¼ã‚’æŠ¼ã™ã¨ã€ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•´å½¢ã—ã¦INBOXã«ä¿å­˜ã—ã¾ã™ã€‚")
    print("çµ‚äº†ã¯ Ctrl+C")
    print("-----------------------------------------------")

    keyboard.add_hotkey(HOTKEY, handle_hotkey)

    try:
        while True:
            time.sleep(0.5)
    except KeyboardInterrupt:
        print("\n[exit] bye")


if __name__ == "__main__":
    main()
