#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import re
import time
import hashlib
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import pyperclip
import keyboard
from janome.tokenizer import Tokenizer


# =========================
# è¨­å®š
# =========================

INBOX_DIR = r"C:\Users\zyaga\OneDrive\Documents\ObsidianVault\00_INBOX"

# INBOXç”¨ï¼šã‚¿ã‚°ã¯æœ€å¤§1ã¤ï¼ˆINBOX or è‡ªå‹•ã‚«ãƒ†ã‚´ãƒª1ã¤ï¼‰
PRIMARY_FALLBACK_TAG = "INBOX"

# å·¦æ‰‹ã§å±Šããƒ›ãƒƒãƒˆã‚­ãƒ¼
HOTKEY = "ctrl+alt+q"

MIN_CHARS = 3

# å±¥æ­´ãƒ¡ãƒ³ãƒ†ï¼ˆè‚¥å¤§åŒ–é˜²æ­¢ï¼‰
DEDUPE_MAX_RECORDS = 2000
DEDUPE_HISTORY_FILENAME = "_clip_history.json"

# ãƒ•ã‚¡ã‚¤ãƒ«åè¦ç´„ã®æœ€å¤§é•·ï¼ˆã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¾Œï¼‰
FILENAME_SUMMARY_MAX = 40

# =========================
# åˆ†é¡ãƒ«ãƒ¼ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¢—ã‚„ã›ã¾ã™ï¼‰
# =========================

CATEGORY_RULES: Dict[str, Dict[str, List[str]]] = {
    "work": {
        "any": [
            "ä¼šè­°", "è­°äº‹éŒ²", "è¦ä»¶", "è¨­è¨ˆ", "ãƒ†ã‚¹ãƒˆ", "éšœå®³", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ä»•æ§˜", "èª²é¡Œ", "ã‚¿ã‚¹ã‚¯",
            "WBS", "é€²æ—", "é¡§å®¢", "ãƒ¦ãƒ¼ã‚¶", "å•ã„åˆã‚ã›", "ãƒªãƒªãƒ¼ã‚¹", "ä¿å®ˆ", "é‹ç”¨",
            "SVN", "Git", "Bitbucket", "Jira", "Confluence", "SharePoint", "OneNote", "Copilot", "Teams",
            "è¦‹ç©", "ç¨¼åƒ", "å·¥æ•°", "PR", "MR", "ãƒ–ãƒ©ãƒ³ãƒ"
        ],
        "all": []
    },
    "shopping": {
        "any": ["è²·ã†", "è³¼å…¥", "æ³¨æ–‡", "Amazon", "æ¥½å¤©", "ä¾¡æ ¼", "ã‚»ãƒ¼ãƒ«", "æ¯”è¼ƒ", "ã‚¯ãƒ¼ãƒãƒ³", "ãƒã‚¤ãƒ³ãƒˆ", "åœ¨åº«"],
        "all": []
    },
    "health": {
        "any": ["ä½“é‡", "ç­‹ãƒˆãƒ¬", "ã‚¸ãƒ ", "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«", "ç¡çœ ", "ç–²ã‚Œ", "ä½“èª¿", "é£Ÿäº‹", "PFC", "ã‚¿ãƒ³ãƒ‘ã‚¯", "ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ"],
        "all": []
    },
    "game": {
        "any": ["ãƒ¢ãƒ³ãƒãƒ³", "ãƒ¢ãƒ³ãƒãƒ³NOW", "Switch", "Steam", "PS5", "æ”»ç•¥", "å‘¨å›", "ãƒ¬ãƒ™ãƒ«", "ã‚¬ãƒãƒ£", "ãƒœã‚¹", "ã‚¯ã‚¨ã‚¹ãƒˆ"],
        "all": []
    },
    "travel": {
        "any": ["æ—…è¡Œ", "ãƒ›ãƒ†ãƒ«", "æ–°å¹¹ç·š", "é£›è¡Œæ©Ÿ", "äºˆç´„", "ãƒ«ãƒ¼ãƒˆ", "è¦³å…‰", "æ¸©æ³‰", "é§…", "ç©ºæ¸¯"],
        "all": []
    },
    "finance": {
        "any": ["å®¶è¨ˆ", "ç¨é‡‘", "ãµã‚‹ã•ã¨ç´ç¨", "ã‚¯ãƒ¬ã‚«", "ãƒã‚¤ãƒ³ãƒˆ", "æ”¯å‡º", "è²¯é‡‘", "æŠ•è³‡"],
        "all": []
    },
    "obsidian": {
        "any": ["Obsidian", "Vault", "Daily", "diary", "ãƒªãƒ³ã‚¯", "[[", "ã‚¿ã‚°", "ãƒ†ãƒ³ãƒ—ãƒ¬", "Second Brain"],
        "all": []
    },
}

# wikilinkã¯ã€Œæœ¬æ–‡ã®èª­ã¿ã‚„ã™ã•å„ªå…ˆã€ã§ä¸Šé™ã¤ã
MAX_WIKILINKS = 12
BASE_WIKILINK_SEEDS = [
    "Obsidian", "Second Brain", "INBOX", "Daily Note", "ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«", "æŒ¯ã‚Šè¿”ã‚Š",
    "æ°—ã¥ããƒ¡ãƒ¢", "ã‚¿ã‚¹ã‚¯ç®¡ç†", "è‡ªå‹•åŒ–", "Python", "Copilot", "Copilot 365",
]

JP_STOPWORDS = set([
    "ã“ã¨", "ã‚‚ã®", "ãã‚Œ", "ã“ã‚Œ", "ãŸã‚", "ã¨ã“ã‚", "æ„Ÿã˜", "è‡ªåˆ†", "ä»Šæ—¥", "æ˜æ—¥",
    "ä»Šå›", "ä¸€æ—¦", "å¿…è¦", "å¯èƒ½", "æ–¹", "æ™‚", "ã‚ã¨", "å‰", "ä¸­", "å¾Œ", "ä¸Š", "ä¸‹",
    "ç§", "åƒ•", "ä¿º", "ã‚ãªãŸ", "ã•ã‚“", "çš„", "ä»–", "ãªã©"
])

tokenizer = Tokenizer()


@dataclass
class DedupeRecord:
    sha256: str
    created_at: str
    filename: str


# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================

def now_str() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def normalize_text(text: str) -> str:
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()


def sha256_of(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def atomic_write_text(path: Path, content: str, encoding: str = "utf-8") -> None:
    tmp = path.with_suffix(path.suffix + ".tmp")
    tmp.write_text(content, encoding=encoding)
    tmp.replace(path)  # atomic on same filesystem


# =========================
# å±¥æ­´ï¼ˆ8: è‡ªå‹•ãƒ¡ãƒ³ãƒ†ï¼‰
# =========================

def load_history(path: Path) -> List[DedupeRecord]:
    if not path.exists():
        return []
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        out: List[DedupeRecord] = []
        for item in data.get("records", []):
            sha = item.get("sha256", "")
            if not sha:
                continue
            out.append(DedupeRecord(
                sha256=sha,
                created_at=item.get("created_at", ""),
                filename=item.get("filename", ""),
            ))
        return out
    except Exception:
        # å£Šã‚Œã¦ã„ã¦ã‚‚èµ·å‹•ã¯ç¶™ç¶šï¼ˆæ–°è¦å±¥æ­´æ‰±ã„ï¼‰
        return []


def maintenance_history(records: List[DedupeRecord]) -> List[DedupeRecord]:
    """
    - åŒä¸€shaã®é‡è¤‡æ’é™¤ï¼ˆæœ€æ–°å„ªå…ˆï¼‰
    - ä¸Šé™ãƒˆãƒªãƒ ï¼ˆæœ«å°¾ãŒæœ€æ–°ã«ãªã‚‹å‰æã§ä¿æŒï¼‰
    """
    seen = set()
    compacted: List[DedupeRecord] = []
    for r in reversed(records):
        if r.sha256 in seen:
            continue
        seen.add(r.sha256)
        compacted.append(r)
    compacted.reverse()

    if len(compacted) > DEDUPE_MAX_RECORDS:
        compacted = compacted[-DEDUPE_MAX_RECORDS:]
    return compacted


def save_history(path: Path, records: List[DedupeRecord]) -> None:
    records = maintenance_history(records)
    payload = {
        "updated_at": now_str(),
        "records": [r.__dict__ for r in records],
    }
    atomic_write_text(path, json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


# =========================
# åˆ†é¡ãƒ»ãƒªãƒ³ã‚¯å€™è£œ
# =========================

def detect_categories(text: str) -> List[str]:
    tags = []
    for tag, rule in CATEGORY_RULES.items():
        any_hits = any(k.lower() in text.lower() for k in rule.get("any", []))
        all_req = rule.get("all", [])
        all_hits = all(k.lower() in text.lower() for k in all_req) if all_req else True
        if any_hits and all_hits:
            tags.append(tag)
    return tags


def choose_primary_tag(auto_categories: List[str]) -> str:
    """
    INBOXç”¨ï¼šã‚¿ã‚°ã¯æœ€å¤§1ã¤ã€‚
    - è‡ªå‹•ã‚«ãƒ†ã‚´ãƒªãŒã€Œ1ã¤ã ã‘ã€å‡ºãŸã¨ãã ã‘æ¡ç”¨
    - ãã‚Œä»¥å¤–ã¯ INBOX
    """
    if len(auto_categories) == 1:
        return auto_categories[0]
    return PRIMARY_FALLBACK_TAG


def extract_english_terms(text: str) -> List[str]:
    candidates = re.findall(r"[A-Za-z0-9][A-Za-z0-9\._\-/\+]{1,}", text)
    out = []
    for c in candidates:
        if len(c) < 3:
            continue
        if re.fullmatch(r"\d+", c):
            continue
        out.append(c)
    return out


def extract_japanese_nouns(text: str) -> List[str]:
    out = []
    for token in tokenizer.tokenize(text):
        base = token.base_form
        pos = token.part_of_speech.split(",")
        if pos[0] != "åè©":
            continue
        if pos[1] in ["æ•°", "éè‡ªç«‹", "ä»£åè©"]:
            continue
        if not base or base in JP_STOPWORDS:
            continue
        if len(base) <= 1:
            continue
        out.append(base)
    return out


def score_terms(text: str, terms: List[str]) -> List[Tuple[str, int]]:
    scored = {}
    for term in terms:
        freq = len(re.findall(re.escape(term), text))
        if freq <= 0:
            continue
        score = freq * 10 + min(len(term), 12)
        scored[term] = max(scored.get(term, 0), score)
    return sorted(scored.items(), key=lambda x: x[1], reverse=True)


def select_wikilinks(text: str) -> List[str]:
    seeds = [s for s in BASE_WIKILINK_SEEDS if s and s.lower() in text.lower()]
    nouns = extract_japanese_nouns(text)
    eng = extract_english_terms(text)

    candidates = list(set(seeds + nouns + eng))
    ranked = score_terms(text, candidates)

    selected = []
    for term, _score in ranked:
        freq = len(re.findall(re.escape(term), text))

        if term not in seeds:
            if len(term) <= 4:
                continue
            if freq < 2:
                continue

        if term in JP_STOPWORDS:
            continue

        selected.append(term)
        if len(selected) >= MAX_WIKILINKS:
            break

    return sorted(set(selected), key=len, reverse=True)


# =========================
# 10: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚«ãƒ†ã‚´ãƒª+è¦ç´„ï¼‰
# =========================

SENSITIVE_PATTERNS = [
    (re.compile(r"[A-Za-z0-9._%+\-]+@[A-Za-z0-9.\-]+\.[A-Za-z]{2,}"), "<email>"),
    (re.compile(r"\b0\d{1,3}-\d{2,4}-\d{3,4}\b"), "<phone>"),
    (re.compile(r"\b\d{4,}\b"), "<num>"),
]


def sanitize_filename_part(s: str) -> str:
    s = s.strip()
    s = re.sub(r"[\\/:*?\"<>|]+", "_", s)
    s = re.sub(r"\s+", " ", s).strip()
    s = s.strip(".")
    return s


def redact_sensitive(s: str) -> str:
    out = s
    for pat, rep in SENSITIVE_PATTERNS:
        out = pat.sub(rep, out)
    return out


def make_summary_for_filename(text: str, fallback_terms: Optional[List[str]] = None) -> str:
    t = normalize_text(text)
    lines = [ln.strip() for ln in t.split("\n") if ln.strip()]
    first = lines[0] if lines else ""

    if first:
        if first.startswith("http") or len(first) < 4:
            first = ""
        if re.search(r"https?://", first):
            first = ""

    if first:
        cand = first
    else:
        terms = fallback_terms or (extract_japanese_nouns(t) + extract_english_terms(t))
        ranked = [w for w, _ in score_terms(t, list(set(terms)))]
        cand = " ".join(ranked[:6]) if ranked else "clip"

    cand = redact_sensitive(cand)
    cand = sanitize_filename_part(cand)

    if len(cand) > FILENAME_SUMMARY_MAX:
        cand = cand[:FILENAME_SUMMARY_MAX].rstrip()

    return cand if cand else "clip"


def choose_primary_category(auto_categories: List[str]) -> str:
    return auto_categories[0] if auto_categories else "misc"


# =========================
# ãƒãƒ¼ãƒˆæœ¬æ–‡ç”Ÿæˆ
# =========================

_HEADING_EMOJI_PREFIX = r"[ğŸ”ğŸ’¡ğŸ§ ğŸ§ªğŸ“Œâœ…âŒâš ï¸ğŸ“¥]"
_re_step = re.compile(r"^(?:\d+[.)]|[â‘ -â‘³]|ğŸ‘‰|â€»|æ³¨[:ï¼š]|æ³¨æ„[:ï¼š])\s*")


def _strip_list_marker(s: str) -> str:
    # "- " "* " "ãƒ»" ãªã©ã®å…ˆé ­è¨˜å·ã‚’è½ã¨ã™ï¼ˆChatGPTã‚³ãƒ”ãƒšå¯¾ç­–ï¼‰
    s = s.lstrip()
    s = re.sub(r"^(?:-\s+|\*\s+|ãƒ»\s*)", "", s)
    return s.strip()


def to_bullets(text: str) -> str:
    """
    INBOXç”¨ã®èª­ã¿ã‚„ã™ã•å„ªå…ˆæ•´å½¢ï¼ˆè§£æ±ºç­–Aï¼‰

    - ä¸»å¼µ/è¦‹å‡ºã—/ç« ã‚¿ã‚¤ãƒˆãƒ«      : `- `
      - è£œè¶³èª¬æ˜/ã‚¹ãƒ†ãƒƒãƒ—/æ„å‘³ã¥ã‘ : `  - `
        - å…·ä½“ä¾‹/åˆ—æŒ™/æ‰‹é †         : `    - `
    """
    lines = text.split("\n")
    out: List[str] = []

    for line in lines:
        raw0 = line.strip()
        if not raw0:
            continue

        # ã™ã§ã«markdownè¦‹å‡ºã—ãŒã‚ã‚‹å ´åˆ
        if raw0.startswith("#"):
            title = raw0.lstrip("#").strip()
            if title:
                out.append(f"- {title}")
            continue

        raw = _strip_list_marker(raw0)
        if not raw:
            continue

        # ç« ã£ã½ã„è¡Œï¼ˆçŸ­ã‚ï¼‹å…ˆé ­ãŒçµµæ–‡å­—/è¨˜å·ï¼‰
        if re.match(rf"^{_HEADING_EMOJI_PREFIX}", raw) and len(raw) <= 80:
            out.append(f"- {raw}")
            continue

        # ã€Œçµè«–ã€ã€Œé‡è¦ã€ãªã©ã€å˜ç™ºã§åŒºåˆ‡ã‚Šã«ãªã‚‹è¡Œã‚’ç« æ‰±ã„
        if len(raw) <= 60 and any(k in raw for k in ["çµè«–", "é‡è¦", "åŸºæœ¬ãƒ«ãƒ¼ãƒ—", "å¤§äº‹", "å‰æ", "åˆ°é”ç‚¹", "æ˜æ—¥ã‚„ã‚‹ã“ã¨", "è¦–ç‚¹"]):
            out.append(f"- {raw}")
            continue

        # ã‚¹ãƒ†ãƒƒãƒ—/è£œè¶³æ‰±ã„
        if _re_step.match(raw):
            out.append(f"  - {raw}")
            continue

        # ãã‚Œä»¥å¤–ã¯å…·ä½“ï¼ˆæ·±ã‚ï¼‰
        out.append(f"    - {raw}")

    return "\n".join(out) if out else "- "


def build_markdown(raw_text: str, tags: List[str], wikilinks: List[str]) -> str:
    dt = datetime.now()
    date_str = dt.strftime("%Y-%m-%d")
    time_str = dt.strftime("%H:%M:%S")

    clean = normalize_text(raw_text)
    bullets = to_bullets(clean)

    tags_yaml = "\n".join([f"  - {t.lstrip('#')}" for t in tags])

    frontmatter = f"""---
created: {date_str} {time_str}
tags:
{tags_yaml}
source: clipboard
---
"""

    if wikilinks:
        link_lines = "\n".join([f"- [[{w}]]" for w in wikilinks])
    else:
        link_lines = "- "

    body = f"""
# ğŸ“¥ INBOXã‚¯ãƒªãƒƒãƒ— ({date_str})

## å†…å®¹
{bullets}

## ğŸ”— ãƒªãƒ³ã‚¯å€™è£œ
{link_lines}

## ãƒ¡ã‚¿
- ä¿å­˜: {date_str} {time_str}
""".lstrip()

    return frontmatter + "\n" + body


def write_note(inbox_dir: Path, md: str, raw_text: str, category: str, summary: str) -> Path:
    inbox_dir.mkdir(parents=True, exist_ok=True)

    dt = datetime.now()
    ts = dt.strftime("%Y-%m-%d_%H-%M-%S")

    category_part = sanitize_filename_part(category)
    summary_part = sanitize_filename_part(summary)

    filename = f"{ts}_{category_part}_{summary_part}.md"
    path = inbox_dir / filename

    i = 1
    while path.exists():
        path = inbox_dir / f"{ts}_{category_part}_{summary_part}_{i}.md"
        i += 1

    path.write_text(md, encoding="utf-8")
    return path


# =========================
# å®Ÿè¡Œï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å±¥æ­´ä¿æŒï¼‰
# =========================

INBOX_PATH = Path(INBOX_DIR)
HISTORY_PATH = INBOX_PATH / DEDUPE_HISTORY_FILENAME

HISTORY_RECORDS: List[DedupeRecord] = []
HISTORY_HASHES = set()


def init_history():
    global HISTORY_RECORDS, HISTORY_HASHES
    INBOX_PATH.mkdir(parents=True, exist_ok=True)

    HISTORY_RECORDS = load_history(HISTORY_PATH)
    HISTORY_RECORDS = maintenance_history(HISTORY_RECORDS)
    HISTORY_HASHES = set(r.sha256 for r in HISTORY_RECORDS)

    save_history(HISTORY_PATH, HISTORY_RECORDS)


def handle_hotkey():
    global HISTORY_RECORDS, HISTORY_HASHES

    try:
        raw = pyperclip.paste()
    except Exception:
        print("[skip] cannot read clipboard")
        return

    if not isinstance(raw, str):
        print("[skip] clipboard is not text")
        return

    raw_norm = normalize_text(raw)
    if len(raw_norm) < MIN_CHARS:
        print("[skip] too short")
        return

    h = sha256_of(raw_norm)
    if h in HISTORY_HASHES:
        print("[skip] duplicate (already saved)")
        return

    auto_categories = detect_categories(raw_norm)
    category = choose_primary_category(auto_categories)

    # tags: æœ€å¤§1ã¤ï¼ˆINBOX or è‡ªå‹•ã‚«ãƒ†ã‚´ãƒª1ã¤ï¼‰
    primary_tag = choose_primary_tag(auto_categories)
    tags = [primary_tag]

    wikilinks = select_wikilinks(raw_norm)

    summary = make_summary_for_filename(raw_norm)

    md = build_markdown(raw_norm, tags, wikilinks)
    saved = write_note(INBOX_PATH, md, raw_norm, category, summary)

    rec = DedupeRecord(sha256=h, created_at=now_str(), filename=saved.name)
    HISTORY_RECORDS.append(rec)
    HISTORY_RECORDS = maintenance_history(HISTORY_RECORDS)
    HISTORY_HASHES = set(r.sha256 for r in HISTORY_RECORDS)
    save_history(HISTORY_PATH, HISTORY_RECORDS)

    print(f"[saved] {saved}")


def main():
    init_history()

    print("=== Clipboard -> Obsidian INBOX (Hotkey Save) ===")
    print(f"INBOX_DIR : {INBOX_DIR}")
    print(f"HOTKEY    : {HOTKEY}")
    print(f"History   : {HISTORY_PATH} (max {DEDUPE_MAX_RECORDS})")
    print("ãƒ›ãƒƒãƒˆã‚­ãƒ¼ã‚’æŠ¼ã™ã¨ã€ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•´å½¢ã—ã¦INBOXã«ä¿å­˜ã—ã¾ã™ã€‚")
    print("çµ‚äº†ã¯ Ctrl+C")
    print("-----------------------------------------------")

    keyboard.add_hotkey(HOTKEY, handle_hotkey)

    try:
        while True:
            time.sleep(0.5)
    except KeyboardInterrupt:
        print("\n[exit] bye")


if __name__ == "__main__":
    main()
